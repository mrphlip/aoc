# 15 &ndash; The one where I broke my computer
Man, I hecked up hard with this one.

I didn't realise quite how **lorge** the dataset area was. So I initially tried to do the [naive solution](https://github.com/mrphlip/aoc/blob/37512e4616ddb3790a42270954dc910d99a06062/2022/15.py) where I just build a map of every point that the sensors have coverage (even though the first part only cared about one `y` level, I assumed, correctly, that part 2 would care about the entire area). I coded that up, set it running, and when it didn't quickly come back with a response I decided I'd leave it running while I coded up a smarter solution, just in case it finished first.

What I didn't realise is that the area is going to be 4-million-by-4-million (and the coverage of the sensors also bleeds quite far outside that area, which my naive approach would also include). I don't know about you, but my PC does not have enough RAM to store a set with over 16 trillion entries. My script quickly allocated something like 25GB of RAM, forced everything else out to swap, and dragged the system to a crawl. I was eventually able to bring up `top` and kill the process and try to recover, but my web browser was unresponsive... I thought it was just taking a while to page back in from swap, but after a minute or two I got impatient and killed the web browser and restarted. Which all was time I wanted to be spending plugging guesses into the AoC website, but couldn't. All my own fault, to be sure, but it's still annoying to lose time to something that isn't solving the coding puzzle (even if it is _caused by_ trying to solve the coding puzzle, badly).

One thing's for sure, that "do it so much more that you need a more efficient solution" that yesterday I thought was coming soon... definitely came today. And just on part 1, at that.

Anyway, my cleverer solution was to just focus on the one `y` level like they wanted, and instead of storing a `set` of every point on that line that has coverage, instead store a list of intervals that are covered (since each sensor, if it covers the line at all, will just cover a single interval of it). And once we have a list of intervals, determining the actual coverage statistics (taking into account that the intervals likely overlap) is not that hard. Not completely trival, but it's all ideas I've worked with before.

I did hit an annoying hiccup where I'd (a) had a small bug in the overlap-processing code that was causing it to _undercount_ the coverage by 1 on the test data, and also (b) not realised I needed to subtract any actual beacons that are on that `y` level from the coverage count, which caused it to _overcount_ the coverage by 1 on the test data. The ultimate off-by-one error, meaning it gave the correct answer for the test data. But, of course, the incorrect answer for my real data. Figured that out pretty quickly and fixed it up, though.

For part 2, since I was already pretty satisfied that my solution to part 1 was quite performant (working with intervals should be a lot faster than working with each point individually)... I decided to go back to the brute-ish methods, and just run the part 1 solution over every `y` level within the specified range, and check each one until we found one that didn't have full coverage. This wasn't the speediest solution, and I was getting pretty impatient as it ran, but it only ultimately took about 30 seconds to come up with the correct answer.

The Haskell reimplementation got a head-start as I already had written a [helper library](../Range.hs) for dealing with these sets of intervals, deduping them and the like. Wrote it for [2020 Day 16](../2020/16.md)... and I acknowledged at the time that I'd kinda overengineered it a bit, but that was just future-proofing as today we used some of those overdone features that weren't needed for the earlier puzzle.

[467/123]
