# 19 &ndash; The one where it got complicated
Oh man, this puzzle.

For the Sunday puzzles, I'm in theory busy at the time the puzzle is released, with a scheduled gaming night. But often I can still try to work on the puzzle, as we wait for everyone to show up, and discuss exactly what we're going to do... I can often get a good 15-20 minutes to work on the leaderboard race, and often that's enough. For this one... I had to hard-out at 25 minutes, and I was _nowhere_. I loaded up the leaderboard page out of curiosity about 15 minutes in, and exactly 1 person had gotten their first star. This one was a challenge.

In principle, this puzzle isn't _that_ hard, you can take each pair of sensors, and try each combination to line them up (every orientation, and translated to align every pair of stars) and see what works. But that's still a fair bit of work to do all of the rotating and aligning code, and doing that to brute-force the full point clouds sounds like it would run rather slowly...

So, we want to be a bit clever about it. To do that, we make some invariants &ndash; measurements that don't change even if a sensor gets moved or rotated. Specifically, we take the vector between a pair of points, and then take the absolute value of each x/y/z component, and then sort the components into ascending order. This operation gives the same result if a sensor is moved (since the _difference_ between two points will stay the same) or if a sensor is rotated (since all that can do is shuffle and/or negate different components of the vector).

We then calculate this invariant for each pair of points in the point cloud for a sensor, which forms a fingerprint of that sensor. If two sensors overlap, there should be a significant intersection of their fingerprints. Since we're guaranteed that two sensors will overlap by at least 12 points, we know that their fingerprints should intersect by at least 66 pairs (= 12 choose 2). But in practice there might be a few less, as multiple pairs within that 12 might have the same difference, so the cutoff I actually use in the code is 50 pairs. If two sensors' fingerprints intersect by less than this, it's pretty unlikely the sensors overlap (and if that assumption turned out to be incorrect, and it failed to solve, can easily just reduce that threshold further).

So, now we have a fairly efficient way to estimate how much any given pair of sensors overlap... it's not perfect, as there can be false positives (unrelated pairs of points with the same difference vector) or false negatives (multiple pairs of points within the overlap with the same difference vector) but generally speaking, the larger the fingerprint intersection, the larger the overlap. So, we sort the pairs of sensors by the size of the intersection, and try to align the largest-intersection pairs first. We can't guarantee that the pair that's first in the list will be a pair we can align correctly, but most of the time it will be, and if not there should be one early in the list. This heuristic means that in general, we should find a combination that works early, before having to try all the combinations that don't work.

Now, once we have our candidate pair of sensors to overlap, we need to try to align them. And, our fingerprint can help here again &ndash; we already know a bunch of pairs of points in each sensor's view that are probably aligned. But, again, it's imperfect &ndash; there can be false positives in there, but even for pairs of points that _should_ be aligned, we don't know which way around, aligning just a single pair of points can be done in two orientations (possibly even more, if multiple components in the difference vector are equal). However, we can do the same trick again &ndash; if the two sensors can be aligned, there'll be one orientation that works to align a large number of the point pairs, but all the false-positive or reverse-direction orientations will probably only align one pair, maybe 2 if you get lucky. So we go through each of the candidate pairs from the fingerprint intersection, and get the set of orientations that would align that pair, and then count up how often each orientation appears... the correct orientation should appear a lot, while all the false orientations only appear once, maybe twice. So, again, we trim out a bunch that only appear once, and then sort by frequency, and try the most frequent orientations first... again making it likely that we find an orientation that works early, and can skip out on trying any of the other options.

Once we've successfully aligned two point clouds, we remove both of those point clouds from our dataset, and replace it with our merged point cloud. Then, we repeat this whole process until all the point clouds are merged into a single dataset. There is some inefficiency here in that we re-calculate the fingerprints each loop for the point clouds that weren't changed, but doing it this way saved a bunch of extra passing-around of extra data, and doesn't waste a _lot_ of time.

For Part 2, we need to do a bit more record-keeping... when we merge two point clouds, we also need to keep track of where the sensors are in each point cloud, and pass that all the way back to the mainline. This also wasn't hard, but did involve doing [a bunch of annoying work](https://github.com/mrphlip/aoc/commit/f11c78ffa9ae9b5fe17f07d6c519342d1559f15c) to pass the extra data through all the affected functions.
