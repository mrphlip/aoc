# 10 &ndash; When the Linear is Not Aligning

This one _hurt_ my brain. I feel like I am missing some relevant piece of info here. I did linear alg at uni and that's served me well in these sorts of puzzles before, but I dropped out before we got into any sort of Diophantine stuff. I'm out of my depth here, trying to figure things out from first principles. I imagine there exists _some_ sort of trick to cracking this sort of puzzle, but I wasn't able to take my linalg knowledge and my number theory knowledge and smush them together and invent some kind of integer-only-linalg theory from scratch. Unacceptable, I know.

OK, so for what I do know: from a linalg point of view, both days of this are essentially the same puzzle, just over a different domain. We are given the results of a bunch of operations, and we know those results combine linearly, and we want to invert the process to determine which operations to pick for a particular result. This is classic linalg stuff: build a matrix, invert it. Only difference is that for the first part we're doing this over ùîΩ‚ÇÇ but for the second part we're doing it over ‚Ñö (we'd love to do it over ‚Ñï but, unfortunately, not a field).

Now, I've implemented [Gauss-Jordan](https://en.wikipedia.org/wiki/Gaussian_elimination) a billion times at this point, I could do it in my sleep. In fact, I've already got an implementation of it [in my utils](../utils/matrix.py)... however, that implementation is only good for doing a complete matrix inversion (ie, on a square matrix, which isn't singular). The matricies we're working on here, are frequently not square, and frequently linearly dependent. So that wasn't that useful here. (Note to self: add a more general Gauss-Jordan impl to the matrix class that just gets as far as it can get, to reduced-row-echelon, for use in cases like this in the future.)

So, part 1, I set to work re-implementing this again. Unfortunately, I took a pretty major shortcut that bit me on part 2... rather than representing the values in the matrix directly as lists of numbers, I represented each row vector as a bitset in a single number... this let me quickly and easily do entire row operations with bitwise operators (eg using `^` to add two vectors together), which was a lot less fiddly. Unfortunately this meant that a bunch of the code needed to be rewritten _back_ to storing lists of numbers once I got to part 2, since we are now working with entire numbers, not just effectively booleans. I've preserved both versions of the code in the repo as [10a.py](10a.py) and [10b.py](10b.py).

So, for example, the first row from the sample puzzle:
```
[...#.] (0,2,3,4) (2,3) (0,4) (0,1,2) (1,2,3,4) {7,5,12,7,2}
```
Looking at just the schematic bit in the parens, we turn this into this matrix:
```
[ 1 0 1 1 1 | 1 0 0 0 0 ]
[ 0 0 1 1 0 | 0 1 0 0 0 ]
[ 1 0 0 0 1 | 0 0 1 0 0 ]
[ 1 1 1 0 0 | 0 0 0 1 0 ]
[ 0 1 1 1 1 | 0 0 0 0 1 ]
```
The way to read this is that each row represents an operation we have available... the left side shows what the result will be, and the right side shows which buttons we need to press to get that result. And, importantly, we can add or subtract two rows from each other, or multiply/divide a row by a constant, and we'll still get a row that makes sense, the combined button presses will give in the combined result. The only difference is that for Part 1 we do this in ùîΩ‚ÇÇ (ie every number is only `0` or `1`, we add with `^` and multiply with `&`) and for Part 2 we just do it as regular numbers.

In Part 1, the matrix reduces to this:
```
[ 1 0 0 0 1 | 0 0 1 0 0 ]
[ 0 1 0 0 1 | 1 0 1 0 1 ]
[ 0 0 1 0 0 | 1 0 0 1 1 ]
[ 0 0 0 1 0 | 0 0 1 1 1 ]
[ 0 0 0 0 0 | 1 1 1 0 0 ]
```
while in Part 2, it reduces to this:
```
[ 1 0 0 0  1 |  0 0  1  0  0 ]
[ 0 1 0 0  1 | -1 0  1  0  1 ]
[ 0 0 1 0 -2 |  1 0 -2  1 -1 ]
[ 0 0 0 1  2 |  0 0  1 -1  1 ]
[ 0 0 0 0  0 | -1 1  1  0  0 ]
```
You may be concerned by seeing negative numbers in there... don't worry about those yet (it gets worse on the real puzzle input, some of them are even fractional) this will be a problem later, but for now just pretend this is allowed.

There are three main features of these results, which we can read directly off the left half of the matrix here. Firstly, there are four columns in each one that are all `0`s except for a single `1`. These are what we're hoping to find... the point of the algorithm here is to get as many of these as possible. These indicate outputs of our system that we can freely control, individually. Like, if we want to activate the first output `7` times, that means we can just multiply the top row by `7` and that will do it... and no matter how many times we add in other rows, it'll still do what we want. In an easier puzzle, the left half of the result would come out as a simple identity matrix, where it's all `0`s with `1`s along the entire diagonal, in which case we would then be done &ndash; we could control each output individually without affecting any of the others, and could read out exactly what inputs we'd need to use to do so.

However, no such luck here, there are two other features we need to contend with. First, the final column. It only has nonzero values on rows that have their solitary `1` in another column. This represents an output we _cannot_ freely control. Once we've chosen our counts for the first 4 outputs, our 5th is determined. That is, _any_ pattern which activates the first 4 outputs with counts of `7,5,12,7` (as we're required to by the puzzle), will _necessarily_ activate the final output `2` times, we can't change that. All we can do is `assert` that the output we get matches what the puzzle tells us we need to get.

The final thing is the last row, where the left side is entirely `0`s. These are our degrees of freedom, here we have one of them. This is a row which we can add or subtract to our result any number of times, and it won't affect our result at all. This is the remaining challenging part: figuring out how many times we need to add each degree-of-freedom row to our result, in order to minimise the total number of button presses. And, for Part 2, maintain the restriction that all the button press counts are non-negative integers.

And so this is what we find. By using the upper rows (in this case, the first 4 rows), we result in a vector **s** which is a solution. And from the lower rows (in this case, the last row) we get some number of vectors **n·µ¢** which have no effect on the result. We can combine **s** plus any linear combination of the **n·µ¢**'s and get a valid solution.

That is, our entire solution space is of the form **s** + a‚ÇÄ**n‚ÇÄ** + a‚ÇÅ**n‚ÇÅ** + ..., for some set of co-efficients a·µ¢.

For Part 1, we can simply check every option. Each a·µ¢ only has two options, either we add it, or we don't... so even if there's, like, 3 or 4 no-op vectors, that only means 8 or 16 options to chew through, that's easy.

for Part 2, though... this is a problem. Firstly, that there are an infinity of different linear combinations of the no-op vectors to consider, and we're not even just talking integers, it could be rational multiples of the no-op vectors too, as long as the final vector that comes out has non-negative integers in every slot. How do we enforce this condition? How do we define the search space? There are almost certainly [answers to these questions](https://en.wikipedia.org/wiki/Diophantine_equation#System_of_linear_Diophantine_equations) but I don't know them. So all I could hope to do is look for clues to limit the search space to something tractable.

The big clue I settled on is that there are usually a lot more buttons available, than we have degrees of freedom... which makes sense, if that wan't the case the problem probably wouldn't be tractable at all even with more advanced math tricks. But this often means thare are limitations in the coverage. If there is a component that only has a non-zero value in _one_ of the no-op vectors, that tells us a lot about what the co-efficient of that no-op vector can be, since that co-efficient is the only variable we have to play with that will affect the final number of times we press that button.

For example, suppose the first component of **s** is `-3`, the first component of **n‚ÇÄ** is `1/2`, and the first component of all the other **n·µ¢**'s are `0`. This means that a‚ÇÄ must be (a) at least 6, or the first component of our result would be negative, and (b) even, or the first component of the result will be not an integer. That's huge! We've restricted this coefficient from being potentally any rational number, to a half-bounded and discrete set, something we can actually iterate over.

But now, suppose, say, the third component of **s** is `10 1/3`, the third component of **n‚ÇÄ** is `-1/3`, and the third component of all the other **n·µ¢**'s are also `0`. This puts another, separate constraint on a‚ÇÄ... now it must be (a) at most 31, and (b) one more than a multiple of 3. We can combine this with our previous restrictions, and now we know that, to satisfy both restrictions, a‚ÇÄ must be (a) between 6 and 31, inclusive and (b) 4 more than a multiple of 6 (thanks, [Chinese remainder theorem](https://en.wikipedia.org/wiki/Chinese_remainder_theorem)). That's a finite set of possibilities! And not even a particularly large one! The only values it could be at this point are: `{10, 16, 22, 28}`, that's it! We can check all of those all day, and twice on Sunday.

Now, is this enough on its own? Can we solve it from this? Or do we need to look further? Because the other components, the ones that appear in _multiple_ no-op vectors, still imply constraints on the co-efficients. But those are much more complex. If I could avoid having to think about those, that would be nice.

Well, I did a quick check through all of the puzzle inputs, and _every_ coefficient in _every_ puzzle had at least one constraint in this way. That's big, it means this is enough to make sure our search space is never just "all of ‚Ñö", our search space is always going to be discrete. Unfortunately, though, not every co-efficient is completely bounded... quite a few of them are only half-bounded, so our search space was still infinite. But the correct answer is probably _close_ to the one boundary we have... the only question is how far away from it do we check before we give up?

As a last-ditch effort, I went for a very broad bound that I could still justify to myself as probably being correct: if, say, the maximum value in our target vector (the largest joltage value we're trying to achieve) is, say, `20`... then that means that there's no button that we would ever need to press more than `20` times (since each button only makes the output values go up). So that _almost_ implies that if we only check the first `20` values for each half-bounded co-efficient, then that probably covers all the valid combinations? I mean, probably not, actually, there are probably edge-cases that violate this, but I'd been working at this for over an hour and a half at this point, I just wanted to try _something_ and hope it worked.

This did have some significant implications for the runtime... since the target values in the actual puzzle input were generally up around a hundred, but each cofficient we had managed to get completely bounded, would only have a small handful of possible values. This meant that for the puzzle rows that only had zero or one half-bounded co-efficient, the script would blow through them in milliseconds. The rows that had two half-bounded coefficients, it would stutter on but still get past quickly. But there were a couple of rows that had _three_ half-bounded co-efficients, and those ones really dragged.

Of the 194 rows in my input data, 192 of them, combined, were solved in about 17 seconds total. One of the rows, which had three half-bounded coefficients, but the maximum joltage value it needed to count up to was only about 75, that row took almost 10s on its own. And then the last remaining row, which had three-half-bounded coefficients and needed to count to over _200_, took over _eight minutes_ on its own, just to solve that one puzzle.

However! Solve it, it still did. And it got the right answer out. And then I took a nap.

[22:36/1:59:26] sub two hours baby
